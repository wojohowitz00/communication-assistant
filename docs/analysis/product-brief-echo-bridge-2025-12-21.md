---
stepsCompleted: [1, 2, 3]
inputDocuments: []
workflowType: 'product-brief'
lastStep: 3
project_name: 'echo-bridge'
user_name: 'richard'
date: '2025-12-21'
---

# Product Brief: echo-bridge

**Date:** 2025-12-21
**Author:** richard

---

## Executive Summary

Echo-bridge is an iOS communication app designed for non-verbal users who want to engage in natural, real-time conversations with full visibility and control. Rather than hiding the composition process behind a screen, echo-bridge makes communication visible—allowing both the user and conversation partner to see text appearing in real-time as it's composed. The app supports multiple input methods (gesture-based virtual keyboard typing, sign language recognition, and text input) with smart features like conversation logging, customizable phrases, and multi-language translation. Echo-bridge eliminates the friction of current text-to-speech solutions by restoring the natural rhythm of human conversation.

---

## Core Vision

### Problem Statement

Non-verbal users relying on current text-to-speech and typing-based communication apps face significant friction in quotidian interactions and professional conversations:

1. **Input Accuracy & Speed**: Traditional onscreen typing is error-prone (fat-finger errors) and requires full text correction before output, creating noticeable lag in conversation flow.

2. **Hidden Composition Process**: Typing on a screen hides the message from the conversation partner. They watch and wait without seeing what's being composed, breaking the natural rhythm and intimacy of face-to-face conversation.

3. **Limited Input Flexibility**: Single input methods (typed text or voice) don't work for all situations or preferences, limiting the ability to communicate naturally in different contexts.

4. **Professional Communication Challenges**: In workplace settings—especially presentations, meetings, and business development—current solutions feel clunky and may undermine credibility and presence.

### Problem Impact

The impact extends beyond convenience:

- **Quotidian Interactions**: Daily conversations with friends and family lose spontaneity and depth because of the technological friction.

- **In-Depth Conversations**: Complex thoughts take too long to express accurately, limiting the user's ability to engage in meaningful dialogue where nuanced ideas matter.

- **Professional Presence**: In meetings and business contexts, the visible struggle with communication tools can diminish perceived authority and capability, even though the user's thinking is sharp.

- **Conversational Equality**: The hidden typing creates an unequal dynamic—the partner is passive observer, not collaborative participant in real-time communication.

### Why Existing Solutions Fall Short

Current apps solve *communication output* but not *communication experience*:

- **Text-to-Speech Apps**: Generate audio but don't address input accuracy or the hidden-screen problem. Output is one-directional.

- **Typing + Prediction**: Still requires looking at a screen; doesn't solve the visibility problem. Conversation partner watches you type, not the text appearing.

- **Speech-to-Text**: Assistive quality is poor; requires full correction before use; still hides composition from the partner.

- **No Conversational Context**: Most apps treat each message in isolation without logging history or supporting natural back-and-forth flow.

None of these create the *real-time, visible, collaborative* conversation experience the user needs.

### Proposed Solution

Echo-bridge restores natural conversation by making the communication process visible and choosing input methods that enhance rather than hide the interaction:

**Core Input Methods:**

1. **Gesture-Based Virtual Keyboard Typing** (Primary)
   - Recognizes finger/hand movements in 3D space as if typing on an invisible keyboard
   - Feels natural because it mirrors physical typing muscle memory
   - Visible to conversation partner—they see your hands moving as text appears
   - Provides real-time feedback of recognized text

2. **Sign Language Recognition**
   - Real-time recognition of sign language (ASL and others)
   - Complements gesture typing for more natural conversational expression
   - Captures nuance and emotional tone that typed text alone cannot

3. **Traditional Text Input**
   - Fallback for when detailed text needs to be entered
   - Autocomplete and phrase prediction for speed

**Core Features:**

- **Real-Time Text Display**: Both user and partner see text appearing as it's composed (not hidden)
- **Conversation Logging**: Automatic history of all interactions with easy recall
- **Customizable Phrases**: Quick access to frequently used phrases and responses
- **Multi-Language Translation**: Seamless translation of composed text for partners who speak different languages
- **Professional Output**: High-quality voice synthesis for meetings and presentations

### Key Differentiators

**1. Real-Time Conversational Visibility**
The fundamental difference from all competitors: both parties see the message being composed in real-time. This restores face-to-face conversation dynamics.

**2. Gesture-Based Input as First-Class Citizen**
Gesture typing isn't a gimmick—it's the primary input method because it's:
- Visible and embodied (uses natural hand movements)
- Fast and accurate (mirrors typing)
- Socially present (user is engaging with the partner, not the device)

**3. Multiple Input Modalities**
Gesture typing + sign language + text input means users can choose the right tool for each moment—natural conversational expression when depth matters, gesture typing for efficient composition.

**4. Designed for Professional Contexts**
Built specifically for 1:1 and small group professional interactions: presentations, meetings, business development. Users maintain credibility and presence because they're *present*, not struggling with tools.

**5. Conversational Continuity**
Logging, phrase history, and translation support build lasting conversational relationships—not one-off messages. Users can reference past conversations and continue threads seamlessly.

---

## Target Users

### Primary User: Richard

**Background & Context:**
Richard is a non-verbal data scientist with a complete laryngectomy, navigating both professional responsibilities and rich personal relationships. He's intellectually sharp, curious, and emotionally engaged with his family and friends. His voice is gone, but his capacity for connection, humor, and presence is undiminished.

**Daily Reality:**
- **Professional:** Data scientist at early-stage fintech startup; communicates with team via Slack and Zoom where async text-based communication is acceptable
- **Personal:** Primary focus is intimate, real-time communication with spouse, kindergarten-aged child, and close friends/family
- **Core Use Contexts:** Playground time, running errands, dinner conversations, casual moments with loved ones

**The Core Problem:**
Current text-to-speech apps force Richard to choose between **communicating and being present**. When he types on a screen:
- He breaks eye contact with his child on the playground
- His attention divides between his message and his spouse's response
- Complex emotions (conflict resolution, vulnerability, humor) can't fit into the typing-correction cycle
- Casual banter is impossible—the friction of typing kills spontaneity
- Audio output can't convey tone, sarcasm, or emotional nuance

This technological friction erodes the quality of his relationships.

**What Richard Needs:**
1. **Hands-Free Input** - Gesture typing allows him to communicate while staying visually present
2. **Eyes on People, Not Screens** - Gesture typing is visible to his conversation partner; he stays engaged
3. **Real-Time Conversational Flow** - Text appears as he gestures; no lag between thought and expression
4. **Emotional Nuance** - Ability to adjust tone, cadence, and enunciation to convey humor, sarcasm, vulnerability
5. **Audio + Text** - Output options that work for different literacy levels (child learns phonetics via audio; spouse reads complex ideas via text)

**Success Vision:**
Richard sitting on a bench at the playground with his child, using echo-bridge to ask "What was your favorite part of school today?" He gestures the question; text appears on his child's screen; audio plays in his voice with natural cadence. His child *sees him looking at her*, not at a phone. They laugh. That's the moment echo-bridge succeeds.

---

### Secondary Users: Conversation Partners (Spouse, Child, Friends, Family)

**The Partner Experience:**
While Richard is the primary user, the people he converses with are critical to echo-bridge's success. They benefit in specific ways:

**Spouse:**
- **Benefit:** Real-time visibility into Richard's thoughts as he composes them (no hidden typing)
- **Benefit:** Emotional nuance comes through (tone, humor, vulnerability) rather than flat text
- **Benefit:** Faster resolution of complex conversations (conflict, feelings, planning)
- **Barrier:** None technical; she simply needs to see the text appearing on her end

**Child (Kindergarten-aged, early reader):**
- **Benefit:** Eye contact and presence from Richard instead of divided attention
- **Benefit:** Audio output helps with phonetic learning while maintaining natural conversation
- **Benefit:** Play and banter become possible again; interaction feels natural, not mediated
- **Barrier:** None technical; simple interface that shows text + plays audio

**Friends & Family:**
- **Benefit:** Casual, rapid banter returns (quick responses, jokes, witty replies)
- **Benefit:** Presence and engagement in group conversations or 1:1 interactions
- **Benefit:** Can understand tone and emotional context (sarcasm, warmth, seriousness)
- **Barrier:** None technical; they just need to see/hear the output naturally

---

### User Journey: Richard at the Playground (MVP Core Scenario)

**Scene:** Richard at a playground with his kindergarten-aged child. He wants to ask about school, comment on what she's doing, play along with her imagination.

**Current State (With Existing TTS Apps):**
1. **Child:** "Daddy, look! I'm climbing!" (Richard's child is showing him something)
2. **Richard:** Looks down at phone, starts typing on screen keyboard ("That's great, honey.")
3. **Child:** Watches Richard type, loses momentum, attention drifts
4. **Richard:** Finishes typing, hits send, audio plays ("That's great, honey.")
5. **Problem:** Eye contact is broken. The moment passes. Richard missed her expression.

**Future State (With Echo-Bridge):**
1. **Child:** "Daddy, look! I'm climbing!" (Richard's child is showing him something)
2. **Richard:** Looks at his child, raises his hand, gestures the question as if typing in the air ("How high can you go?")
3. **Text appears on Richard's phone** (and can be shared on child's screen if needed)
4. **Audio plays** in Richard's voice with warm, encouraging tone
5. **Child:** Hears his response, sees him watching her, continues climbing while talking
6. **Richard:** Maintains eye contact, responds with gesture, "Play along": "You're like a superhero!"
7. **Child:** Laughs, keeps playing, keeps talking
8. **Outcome:** Presence, connection, natural banter. The moment is *lived*, not interrupted.

**Journey Phases:**

**Discovery & Onboarding:**
- Richard learns about echo-bridge through personal networks or disability/accessibility communities
- First use: sets up gesture typing, adjusts audio settings for his voice
- Aha moment: Realizes he can respond to his child without looking away

**Core Usage:**
- Daily playground time, errands, home conversations
- Gesture typing becomes natural (mirrors physical typing muscle memory)
- Sign language integration for emotional expression
- Conversation logging captures special moments with family

**Success Metrics (For Richard):**
- Decreased time looking at phone during conversations
- Increased ability to engage in casual banter and jokes
- Better conflict resolution with spouse (complex thoughts expressed faster)
- Child's positive feedback ("Daddy's looking at me more")
- Emotion/humor comes through in conversations (not flat TTS output)

**Long-Term Vision:**
Echo-bridge becomes invisible—Richard stops thinking about "using communication app" and just... communicates. With presence, with humor, with himself fully engaged.

---

## MVP Scope

### Core Features (MVP-Essential)

**Input & Recognition:**
- **Gesture-Based Virtual Keyboard Typing** - Recognizes hand/finger movements in 3D space as if typing on an invisible keyboard; provides real-time feedback of recognized text
- **Traditional Text Input** - Fallback input method for when gesture typing isn't practical; includes autocomplete and basic phrase suggestions

**Output & Display:**
- **Real-Time Text Display** - Text appears on-screen as Richard gestures; can be shared with conversation partner's device in real-time
- **Audio Output (TTS)** - High-quality text-to-speech synthesis; uses natural voice (speaker only, no custom voice variants in MVP)
- **Multi-Language Translation** - Seamless translation of composed text into partner's preferred language before output

**Conversation Management:**
- **Conversation Logging** - Automatic capture and storage of all conversations with timestamps; enables easy retrieval of past interactions
- **Basic Phrase Management** - Quick access to pre-set common phrases (e.g., "How was your day?", "I love you", "Let's talk later"); simple add/edit/delete functionality

**Platform & Technical:**
- **iOS Support** - Both iPhone and iPad
- **Offline Functionality** - Core gesture recognition and TTS work without internet connection; syncing occurs when connectivity returns
- **Voice Cloning Foundation** - Basic setup to record/train on Richard's voice for natural audio output (voice customization reserved for future)

---

### Out of Scope for MVP

**Deferred Features:**
- **Voice Customization** - Multiple voice options, prosody adjustment (tone, cadence, emotion), custom enunciation; essential but technically complex for MVP
- **Sign Language Recognition** - Real-time ASL/other sign language recognition; powerful but requires significant ML model development and testing
- **Advanced Phrase Customization** - Complex phrase libraries, contextual phrase suggestions, phrase templates; basic version ships, advanced features deferred
- **Multi-Language TTS Voices** - TTS will translate but output will use default voice; custom voices per language deferred
- **Headphone/Audio Output Options** - MVP uses speaker output only; headphone support, audio routing, Bluetooth speaker support deferred
- **Cloud Sync & Backup** - MVP is local storage only; cloud sync for cross-device access deferred
- **Integration with Third-Party Apps** - No Slack, Teams, email integration in MVP; integration ecosystem deferred
- **Android/Cross-Platform** - iOS only for MVP; Android, web, desktop deferred

---

### MVP Success Criteria

**User Success Indicators:**
- Richard can complete a full conversation with spouse/child using gesture typing without looking at the phone
- Gesture recognition accuracy reaches 90%+ for common English words and phrases
- Audio output is intelligible and natural-sounding (not robotic)
- Conversation logging captures interactions accurately for later review
- Richard perceives meaningful reduction in friction compared to current TTS apps

**Technical Success:**
- Gesture recognition functions reliably in varied lighting conditions (indoor, outdoor playground)
- Offline functionality works seamlessly; no dropped text or failed translations
- App performance is smooth; no noticeable latency between gesture and text appearance
- Battery usage is acceptable for 4+ hours of active use

**Go/No-Go Decision Points:**
- If gesture recognition accuracy drops below 80%, explore technical improvements before broader testing
- If spouse feedback indicates the app isn't solving the presence/connection problem, re-evaluate core design
- If offline functionality fails frequently, shift to online-first approach
- Success criterion for proceeding beyond MVP: You report meaningful improvement in daily interactions with family + spouse confirms seeing improved presence

---

### Future Vision (Post-MVP Roadmap)

**Phase 2: Enhanced Gesture & Voice**
- Voice customization: Prosody control, emotion adjustment, custom enunciation
- Sign language recognition as primary input option (complementing gesture typing)
- Advanced phrase management with contextual suggestions

**Phase 3: Ecosystem Expansion**
- Android platform
- Web app for partner devices (allowing real-time text sharing on any device)
- Cloud sync for seamless cross-device access
- Integration with popular communication platforms (Slack, Teams, email)

**Phase 4: Beyond MVP User Base**
- Expand to other non-verbal users (ALS, cerebral palsy, speech disorders)
- Professional marketplace: certified interpreters, speech-language pathologists using echo-bridge
- Accessibility beyond non-verbal: hearing-impaired users, dyslexic users, language learners

**Long-Term Vision (2-3 Years):**
Echo-bridge becomes the standard communication platform for non-verbal individuals—trusted, invisible, present. Used by thousands across multiple platforms. Recognized not just as assistive tech, but as transformative communication technology that restores presence and humanity to conversations.
